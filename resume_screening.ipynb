{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f521090e-0c6f-4300-a283-ad8029a83fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.1/40.1 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g.subramanian\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "   ---------------------------------------- 0.0/483.4 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 430.1/483.4 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 483.4/483.4 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB 7.9 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/11.6 MB 8.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/11.6 MB 7.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 7.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 7.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.9/11.6 MB 7.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.3/11.6 MB 7.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.6 MB 7.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/11.6 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.3/11.6 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.5/11.6 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/11.6 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/11.6 MB 7.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.8/11.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.1/11.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.4/11.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.8/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.1/11.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.5/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.4/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.7/11.6 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.0/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.1/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.5/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.8/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.8/11.6 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.2/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "   ---------------------------------------- 0.0/563.4 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 307.2/563.4 kB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  563.2/563.4 kB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 563.4/563.4 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.2 kB ? eta -:--:--\n",
      "   ---------------------------------------  317.4/320.2 kB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 320.2/320.2 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.7 MB 18.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.1/2.7 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.5/2.7 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.8/2.7 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.7 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.4/2.7 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 7.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.0\n",
      "    Uninstalling huggingface-hub-0.33.0:\n",
      "      Successfully uninstalled huggingface-hub-0.33.0\n",
      "Successfully installed huggingface-hub-0.35.0 safetensors-0.6.2 sentence-transformers-5.1.0 tokenizers-0.22.1 transformers-4.56.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install sentence-transformers\n",
    "!pip install PyPDF2 # For reading .pdf files\n",
    "!pip install python-docx # For reading .docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ceb4065-ab51-40b3-baec-2d370f39456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded both datasets!\n",
      "\n",
      "--- First 5 Resumes ---\n",
      "         ID                                         Resume_str  \\\n",
      "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
      "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
      "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
      "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
      "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
      "\n",
      "                                         Resume_html Category  \n",
      "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "4  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "\n",
      "--- First 5 Job Descriptions ---\n",
      "   Unnamed: 0             Job Title  \\\n",
      "0           0     Flutter Developer   \n",
      "1           1      Django Developer   \n",
      "2           2      Machine Learning   \n",
      "3           3         iOS Developer   \n",
      "4           4  Full Stack Developer   \n",
      "\n",
      "                                     Job Description  \n",
      "0  We are looking for hire experts flutter develo...  \n",
      "1  PYTHON/DJANGO (Developer/Lead) - Job Code(PDJ ...  \n",
      "2  Data Scientist (Contractor)\\n\\nBangalore, IN\\n...  \n",
      "3  JOB DESCRIPTION:\\n\\nStrong framework outside o...  \n",
      "4  job responsibility full stack engineer – react...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ---- 1. Define the correct paths to the CSV files ----\n",
    "# The path is 'folder_name/file_name.csv'\n",
    "resume_path = 'Resume.csv/Resume.csv'\n",
    "jobs_path = 'job_title_des.csv/job_title_des.csv' # Use the exact filename from your folder\n",
    "\n",
    "# ---- 2. Load the datasets ----\n",
    "try:\n",
    "    resume_df = pd.read_csv(resume_path)\n",
    "    jobs_df = pd.read_csv(jobs_path)\n",
    "\n",
    "    print(\"✅ Successfully loaded both datasets!\")\n",
    "\n",
    "    # ---- 3. Display the first few rows to inspect the data ----\n",
    "    print(\"\\n--- First 5 Resumes ---\")\n",
    "    print(resume_df.head())\n",
    "\n",
    "    print(\"\\n--- First 5 Job Descriptions ---\")\n",
    "    print(jobs_df.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: Could not find the file.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    print(\"\\nPlease double-check that the file and folder names in the script match what you have.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fc967e-8cb4-4934-a323-7db4619de0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\G.SUBRAMANIAN\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading the Sentence Transformer model... (This may take a moment on the first run)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c676d9a3f822495faa48bc25b7c86b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G.SUBRAMANIAN\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\G.SUBRAMANIAN\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d50380b22484fc985e81b287f1d11ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076b0f7cbf144991855eb201b79fc454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae75b0dcde2542d0952b7efba9f8c95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9561c44c73045f8a7c0142177a1f25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2108bbf333c14860b02cb0fa13b2bb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620c7413f2fc4f3790cff7e7fc97056d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c624123ac74bd0bfc35b7e0bf06a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936e33e0086543f29535c4d268f6e42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb2de3dd6854a8eabc7f94e85e481e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7127e782bda044348f45ece38bc56364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully.\n",
      "\n",
      "Generating embeddings for all resumes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e232e6b28fa34f0b947574fbbad920a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated embeddings for 2484 resumes.\n",
      "\n",
      "Generating embeddings for all job descriptions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18eb08aad0c346ad8c22d2c85890e1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated embeddings for 2277 job descriptions.\n",
      "\n",
      "✅ Embeddings saved to 'resume_embeddings.npy' and 'job_embeddings.npy'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# ---- 4. Generate Embeddings ----\n",
    "\n",
    "# 1. Load a pre-trained Sentence Transformer model\n",
    "# 'all-MiniLM-L6-v2' is a great model that balances speed and performance.\n",
    "print(\"Loading the Sentence Transformer model... (This may take a moment on the first run)\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "\n",
    "# 2. Prepare the text from our dataframes into lists\n",
    "# It's a good practice to handle any potential missing values (NaNs) by filling them with an empty string.\n",
    "resume_texts = resume_df['Resume_str'].fillna('').tolist()\n",
    "job_texts = jobs_df['Job Description'].fillna('').tolist()\n",
    "\n",
    "\n",
    "# 3. Generate the embeddings for resumes\n",
    "print(\"\\nGenerating embeddings for all resumes...\")\n",
    "# The model.encode() function does the heavy lifting of converting text to vectors.\n",
    "# We'll show a progress bar to track the process.\n",
    "resume_embeddings = model.encode(resume_texts, show_progress_bar=True)\n",
    "print(f\"✅ Generated embeddings for {len(resume_embeddings)} resumes.\")\n",
    "\n",
    "\n",
    "# 4. Generate the embeddings for job descriptions\n",
    "print(\"\\nGenerating embeddings for all job descriptions...\")\n",
    "job_embeddings = model.encode(job_texts, show_progress_bar=True)\n",
    "print(f\"✅ Generated embeddings for {len(job_embeddings)} job descriptions.\")\n",
    "\n",
    "\n",
    "# 5. (Optional but Recommended) Save the embeddings to files\n",
    "# This saves a lot of time! You won't have to re-generate the embeddings every time you run the notebook.\n",
    "np.save('resume_embeddings.npy', resume_embeddings)\n",
    "np.save('job_embeddings.npy', job_embeddings)\n",
    "print(\"\\n✅ Embeddings saved to 'resume_embeddings.npy' and 'job_embeddings.npy'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba436d8f-5650-4e44-b20d-6d9e5bd7a121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed embeddings...\n",
      "✅ Embeddings loaded successfully.\n",
      "\n",
      "Calculating cosine similarity matrix...\n",
      "✅ Similarity matrix created with shape: (2484, 2277)\n",
      "\n",
      "--- Top 5 Job Matches for Resume #0 ---\n",
      "\n",
      "Original Resume Category: HR\n",
      "✅ Match: 64.66% | Job Title: Database Administrator\n",
      "✅ Match: 64.42% | Job Title: Database Administrator\n",
      "✅ Match: 57.51% | Job Title: Database Administrator\n",
      "✅ Match: 56.66% | Job Title: Database Administrator\n",
      "✅ Match: 56.34% | Job Title: Database Administrator\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# ---- Step 5: Calculate Cosine Similarity ----\n",
    "\n",
    "# 1. Load the embeddings we saved earlier\n",
    "# This is much faster than re-generating them!\n",
    "print(\"Loading pre-computed embeddings...\")\n",
    "try:\n",
    "    resume_embeddings = np.load('resume_embeddings.npy')\n",
    "    job_embeddings = np.load('job_embeddings.npy')\n",
    "    print(\"✅ Embeddings loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Could not find .npy files. Please re-run Step 4 to generate embeddings.\")\n",
    "    # You might want to stop the script here if the files are missing\n",
    "    # For now, we'll assume they exist.\n",
    "\n",
    "# 2. Calculate the cosine similarity matrix\n",
    "# This compares every resume embedding with every job embedding.\n",
    "print(\"\\nCalculating cosine similarity matrix...\")\n",
    "# The result is a matrix where matrix[i, j] is the similarity\n",
    "# between the i-th resume and the j-th job.\n",
    "cosine_sim_matrix = cosine_similarity(resume_embeddings, job_embeddings)\n",
    "print(f\"✅ Similarity matrix created with shape: {cosine_sim_matrix.shape}\")\n",
    "\n",
    "\n",
    "# ---- Step 6: Rank and Present Results ----\n",
    "\n",
    "# Let's find the best job matches for a specific resume.\n",
    "# We'll use the first resume in our dataset as an example (index 0).\n",
    "resume_index = 0\n",
    "\n",
    "# 1. Get the similarity scores for our chosen resume against all jobs.\n",
    "resume_scores = cosine_sim_matrix[resume_index]\n",
    "\n",
    "# 2. Get the indices of the top N job matches.\n",
    "# We'll use np.argsort to get the indices that would sort the array,\n",
    "# then we take the last 5 and reverse them to get the top 5.\n",
    "top_n = 5\n",
    "top_job_indices = np.argsort(resume_scores)[-top_n:][::-1]\n",
    "\n",
    "# 3. Display the results\n",
    "print(f\"\\n--- Top {top_n} Job Matches for Resume #{resume_index} ---\")\n",
    "\n",
    "# Get the text of the resume we are matching\n",
    "# Using .iloc[resume_index] to get the specific resume's text\n",
    "resume_text = resume_df['Resume_str'].iloc[resume_index]\n",
    "print(f\"\\nOriginal Resume Category: {resume_df['Category'].iloc[resume_index]}\")\n",
    "# Optional: Print a snippet of the resume\n",
    "# print(f\"Resume Snippet: {resume_text[:200]}...\\n\")\n",
    "\n",
    "\n",
    "for index in top_job_indices:\n",
    "    # Get the job title and description from the original jobs dataframe\n",
    "    job_title = jobs_df['Job Title'].iloc[index]\n",
    "    \n",
    "    # Get the similarity score and convert it to a percentage\n",
    "    match_score = resume_scores[index] * 100\n",
    "    \n",
    "    print(f\"✅ Match: {match_score:.2f}% | Job Title: {job_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b210186-2c6c-4b4e-8324-dfaf07124f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
